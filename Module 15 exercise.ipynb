{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dd0ee-f091-4ee4-8b63-4a50657314bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Finding the 10 Most Frequent Words in \"Romeo and Juliet\"\n",
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "def find_most_common_words(text, num_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    most_common = word_counts.most_common(num_words)\n",
    "    return most_common\n",
    "\n",
    "romeo_and_juliet_url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "text = get_text_from_url(romeo_and_juliet_url)\n",
    "most_common_words = find_most_common_words(text, 10)\n",
    "print(most_common_words)\n",
    "\n",
    "# Task 2: Analyzing the Cats API\n",
    "import requests\n",
    "import statistics\n",
    "\n",
    "def get_cats_data(url):\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "cats_data = get_cats_data(cats_api)\n",
    "\n",
    "# I. Weight Analysis\n",
    "weights = [float(cat['weight']['metric'].split(' - ')[0]) for cat in cats_data]\n",
    "min_weight = min(weights)\n",
    "max_weight = max(weights)\n",
    "mean_weight = statistics.mean(weights)\n",
    "median_weight = statistics.median(weights)\n",
    "std_dev_weight = statistics.stdev(weights)\n",
    "\n",
    "print(f\"Weight (kg) - Min: {min_weight}, Max: {max_weight}, Mean: {mean_weight}, Median: {median_weight}, Std Dev: {std_dev_weight}\")\n",
    "\n",
    "# II. Lifespan Analysis\n",
    "lifespans = [float(cat['life_span'].split(' - ')[0]) for cat in cats_data]\n",
    "min_lifespan = min(lifespans)\n",
    "max_lifespan = max(lifespans)\n",
    "mean_lifespan = statistics.mean(lifespans)\n",
    "median_lifespan = statistics.median(lifespans)\n",
    "std_dev_lifespan = statistics.stdev(lifespans)\n",
    "\n",
    "print(f\"Lifespan (years) - Min: {min_lifespan}, Max: {max_lifespan}, Mean: {mean_lifespan}, Median: {median_lifespan}, Std Dev: {std_dev_lifespan}\")\n",
    "\n",
    "# III. Frequency Table of Country and Breed\n",
    "from collections import defaultdict\n",
    "\n",
    "country_breed_freq = defaultdict(list)\n",
    "for cat in cats_data:\n",
    "    country_breed_freq[cat['origin']].append(cat['name'])\n",
    "\n",
    "for country, breeds in country_breed_freq.items():\n",
    "    print(f\"{country}: {len(breeds)} breeds\")\n",
    "\n",
    "# Task 3: Analyzing the Countries API\n",
    "import requests\n",
    "\n",
    "def get_countries_data(url):\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "countries_api = 'https://restcountries.com/v3.1/all'\n",
    "countries_data = get_countries_data(countries_api)\n",
    "\n",
    "# I. 10 Largest Countries by Area\n",
    "largest_countries = sorted(countries_data, key=lambda x: x['area'], reverse=True)[:10]\n",
    "print(\"10 Largest Countries by Area:\")\n",
    "for country in largest_countries:\n",
    "    print(f\"{country['name']['common']}: {country['area']} kmÂ²\")\n",
    "\n",
    "# II. 10 Most Spoken Languages\n",
    "from collections import Counter\n",
    "\n",
    "languages = [lang for country in countries_data for lang in country['languages'].values()]\n",
    "most_spoken_languages = Counter(languages).most_common(10)\n",
    "print(\"10 Most Spoken Languages:\")\n",
    "print(most_spoken_languages)\n",
    "\n",
    "# III. Total Number of Languages\n",
    "total_languages = len(set(languages))\n",
    "print(f\"Total Number of Languages: {total_languages}\")\n",
    "\n",
    "# Task 4: Reading UCI Machine Learning Repository\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_uci_datasets(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    datasets = soup.find_all('a', href=True)\n",
    "    return [dataset.text for dataset in datasets if 'datasets' in dataset['href']]\n",
    "\n",
    "uci_url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "uci_datasets = get_uci_datasets(uci_url)\n",
    "print(uci_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
